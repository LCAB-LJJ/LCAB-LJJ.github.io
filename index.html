<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"lcab-ljj.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="gpt-4">
<meta property="og:type" content="website">
<meta property="og:title" content="AIGC-platform">
<meta property="og:url" content="https://lcab-ljj.github.io/index.html">
<meta property="og:site_name" content="AIGC-platform">
<meta property="og:description" content="gpt-4">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="LCAB-LJJ">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://lcab-ljj.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>AIGC-platform</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">AIGC-platform</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">AIGC</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LCAB-LJJ</p>
  <div class="site-description" itemprop="description">gpt-4</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lcab-ljj.github.io/2024/07/24/%E7%8C%9C%E6%83%B3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LCAB-LJJ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIGC-platform">
      <meta itemprop="description" content="gpt-4">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AIGC-platform">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/24/%E7%8C%9C%E6%83%B3/" class="post-title-link" itemprop="url">【最新最热】几何朗兰兹猜想终获破解！跨越30年研究，800余页证明论文问世，中国学者陈麟卓越贡献！</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-07-24 17:53:01 / 修改时间：18:02:46" itemprop="dateCreated datePublished" datetime="2024-07-24T17:53:01+08:00">2024-07-24</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>历经三十余载的潜心研究，九位杰出的数学家携手跨越了学术的崇山峻岭，通过五篇累计超过八百页的深邃论文，终于迎来了数学史上的一大盛事——<strong>几何朗兰兹猜想</strong>的壮丽证明！</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/1.jpg"><br>这一里程碑式的成就，正是朗兰兹纲领几何化版本的璀璨绽放。朗兰兹纲领，作为现代数学研究的璀璨明珠，被誉为“数学的大统一理论”，它深刻地揭示了数论、代数几何与群表示论这三个看似独立却又紧密相连的数学分支之间的内在关联。</p>
<p>费马大定理的完全证明，便是朗兰兹纲领威力的一次绝佳展示。安德鲁·威尔斯（Andrew Wiles）教授通过对数论朗兰兹关系中一小部分函数的精妙证明，成功解锁了困扰数学界长达三个世纪的古老谜题。</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/2.jpg"><br>而几何朗兰兹猜想，作为朗兰兹纲领在几何领域的璀璨延伸，自上世纪80年代被提出以来，便承载着数学家们将数论智慧融入几何世界的梦想。它如同一座桥梁，不仅让数论的方法与概念在几何的舞台上大放异彩，也为几何问题提供了新的求解路径。</p>
<p>该猜想的成功证明，无疑为数学与物理领域的众多未解之谜开辟了新的探索路径。无论是量子场论的深奥探索，还是弦理论的微妙构建，几何朗兰兹猜想都为其提供了宝贵的思路与工具。</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/3.jpg"><br>菲尔兹奖得主彼得·舒尔茨（Peter Scholze）对此赞不绝口，将这一成就誉为“三十年努力的巅峰”。而几何朗兰兹纲领的奠基人之一，亚历山大·贝林森（Alexander Beilinson）更是激动地表示：“这个证明美得令人窒息，无疑是同类证明中的巅峰之作！”</p>
<p>在这一伟大征程中，丹尼斯·盖茨戈里（Dennis Gaitsgory）与山姆·拉斯金（Sam Raskin）作为领军人物，带领着包括中国学者陈麟在内的九人精英团队，共同攀登了这座数学的高峰。陈麟教授，作为清华大学丘成桐数学科学中心的杰出青年才俊，其辉煌的学术生涯早在15岁摘得IMO金牌时便已初露锋芒。</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/4.jpg"><br>几何：朗兰兹纲领的完美闭环</p>
<p>时间回溯至1967年，年仅30岁的普林斯顿大学教授罗伯特·朗兰兹（Robert Langlands）以一封长达17页的手写信，向“数学的罗塞塔石碑”的缔造者安德烈·韦尔（André Weil）描绘了他的宏伟蓝图。这封信，不仅标志着朗兰兹纲领的诞生，更预示着数学界一场深刻变革的到来。</p>
<p>（注：“罗塞塔石碑”在此为喻，意在强调安德烈·韦尔提出的数学领域间类比的重要性，它如同古埃及的罗塞塔石碑一样，为解读不同数学分支间的联系提供了关键线索。）</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/5.jpg"><br>朗兰兹在信中大胆预言，在数论与函数域的广阔天地中，存在着类似于傅里叶分析那样深刻的推广。傅里叶分析，这一将复杂波形转化为平滑振荡三角函数波的技术，不仅是现代电信、信号处理等领域的基石，更为朗兰兹构建数学统一框架提供了灵感。</p>
<p>朗兰兹纲领正是通过在这三个数学分支间建立类似“对应关系”的桥梁，实现了它们之间的无缝连接。在这一框架下，“波”与“频谱”的概念被赋予了新的数学意义：特殊函数构成了“波”的海洋，而代数对象则如同“频谱”一般，精准地标记着“波”的频率与特性。</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/6.jpg"><br>正是这样的统一视角，让朗兰兹纲领成为了一把解锁数学难题的金钥匙。它不仅让许多传统数论问题得以转化为表示论或其他领域的问题，从而找到新的解决途径，更在费马大定理等经典难题的证明中发挥了关键作用。此外，朗兰兹纲领的思想与方法还深刻影响了物理学等其他学科的发展，为量子场论和弦理论等前沿领域的研究提供了宝贵的启示。</p>
<p>而几何朗兰兹猜想，作为朗兰兹纲领中最为璀璨的一环，其成功证明不仅为数学界带来了前所未有的震撼与喜悦，更为未来的数学研究开辟了无限可能。</p>
<p>在证明几何朗兰兹猜想的壮丽征途中，核心策略如同一座灯塔，指引着数学家们穿越未知的数学海洋。他们致力于构建一个等价桥梁，将代数曲线X上G-丛的D-模范畴与朗兰兹对偶群 ^的局部系统Ind-Coh范畴紧密相连。这一等价关系的建立，不仅是数学抽象概念的碰撞，更是对几何与代数深刻联系的洞察。</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/7.jpg"><br>时间回溯至2013年，丹尼斯·盖茨戈里以其敏锐的直觉，勾勒出了几何朗兰兹猜想证明的初步蓝图。然而，这座宏伟建筑的基石——众多中间结果的证明，尚待时日。随后的数年间，丹尼斯与他的合作者们披荆斩棘，逐一攻克这些难题，为最终的胜利奠定了坚实的基础。</p>
<p>2020年，丹尼斯的思维火花再次闪耀，他开始深入探索每个特征层对“白噪声”的独特贡献。这一创新视角，灵感源自傅里叶变换中正弦波的角色，巧妙地将庞加莱层引入朗兰兹猜想的框架之中。</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/8.jpg"><br>2022年的春天，山姆·拉斯金及其学生乔阿基姆·费尔格曼的突破性发现，如同春风化雨，为证明工作注入了新的活力。他们证明了每个特征层都以某种精妙的方式，为“白噪声”的构成添砖加瓦。这一关键进展，让丹尼斯信心倍增，预示着胜利的曙光即将来临。</p>
<p>自2023年起，丹尼斯、山姆及其七位杰出的合作者，携手向几何朗兰兹猜想的最终胜利发起了冲锋。经过无数个日夜的辛勤耕耘，他们终于完成了这项史诗般的任务。五篇累计超过八百页的论文，如同五座巍峨的丰碑，矗立在数学史的长河之中，见证了这一伟大时刻的到来。</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/9.jpg"><br>首篇论文聚焦于函子（functor）的精心构造，在特征为零的优越环境下，成功地从自守领域跨越至谱的彼岸，构建了几何朗兰兹函子LG，并证明了其等价性。这一成就，如同在两座数学山峰之间架起了一座坚固的桥梁，让几何朗兰兹猜想的轮廓逐渐清晰。</p>
<p>第二篇论文则深入探索了Kac-Moody定位与全局的微妙互动，进一步巩固了LG函子作为等价性函子的地位。这一发现，不仅加深了数学家们对几何朗兰兹猜想的理解，更为后续的研究开辟了新的道路。</p>
<p>第三篇论文作为连接已知与未知的桥梁，不仅将等价性结果拓展至更广泛的领域，还通过Kac-Moody局部化技术，揭示了几何朗兰兹函子与常数项函子之间的和谐共舞。同时，对可约谱参数下几何朗兰兹猜想兼容性的证明，为攻克不可约谱参数下的难题奠定了坚实的基础。</p>
<p>第四篇论文则聚焦于Ambidexterity定理的辉煌证明。这一定理犹如一把钥匙，打开了通往LG函子等价性证明的大门。它表明LG-cusp的左伴随与右伴随在特定条件下是同构的，这一发现对于证明LG的等价性具有至关重要的意义。</p>
<p>最后，第五篇论文将上述所有成果汇聚一堂，以雷霆万钧之势将几何朗兰兹猜想推广至一般情况。这一里程碑式的成就，不仅标志着长达三十余年努力的圆满结束，更为数学界带来了前所未有的震撼与喜悦。</p>
<p><img src="/images/loading.png" data-original="/images/%E7%8C%9C%E6%83%B3%E8%AF%81%E6%98%8E/10.jpg"><br>在这场跨越时代的数学盛宴中，哈佛大学教授丹尼斯·盖茨戈里与耶鲁大学教授山姆·拉斯金无疑是耀眼的双子星。然而，他们并非孤军奋战，研究团队中还包括了来自世界各地的杰出数学家。其中，中国学者陈麟的参与尤为引人注目。作为清华大学丘成桐数学科学中心的助理教授，陈麟不仅在数学领域展现出了非凡的才华，更在几何朗兰兹纲领的研究中留下了深刻的印记。</p>
<p>陈麟的成长历程，是天赋与努力的完美结合。从少年时期的数学奥林匹克金牌得主，到哈佛大学博士毕业并荣获优秀奖学金，他的每一步都走得坚定而扎实。在丹尼斯的引领下，他深入几何朗兰兹纲领的研究领域，为证明几何朗兰兹猜想贡献了自己的力量。未来，他将继续在数学的道路上探索前行，为攀登更高的数学高峰而不懈努力。</p>
<p>此外，中国数学界对朗兰兹纲领的关注和投入也令人瞩目。北大黄金一代的恽之玮、张伟、袁新意、朱歆文等学者，正以他们的智慧和汗水，为这一宏伟的数学蓝图添砖加瓦。可以预见，在未来的日子里，中国数学学者将在朗兰兹纲领的研究中取得更多令人瞩目的成就。</p>
<p>你好！我是LCAB-LJJ，一名关注AIGC技术的博主，如果你也喜欢我的文章就看看博主其他文章博主宝藏小站，开通正版GPT-4o教程在教程</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lcab-ljj.github.io/2024/07/23/hongkong/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LCAB-LJJ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIGC-platform">
      <meta itemprop="description" content="gpt-4">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AIGC-platform">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/23/hongkong/" class="post-title-link" itemprop="url">【包教包会】如何在OnlyFans订阅HongkongDoll？ OnlyFans如何订阅？使用虚拟信用卡订阅OnlyFans教程</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-07-23 15:51:08 / 修改时间：16:30:19" itemprop="dateCreated datePublished" datetime="2024-07-23T15:51:08+08:00">2024-07-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="onlyfans-如何使用虚拟卡进入充值，一文教你订阅博主（最新图文教程）"><a href="#onlyfans-如何使用虚拟卡进入充值，一文教你订阅博主（最新图文教程）" class="headerlink" title="onlyfans 如何使用虚拟卡进入充值，一文教你订阅博主（最新图文教程）"></a>onlyfans 如何使用虚拟卡进入充值，一文教你订阅博主（最新图文教程）</h2><p>大家好，OnlyFans是一个非常受欢迎的内容订阅服务平台，它允许创作者通过粉丝的订阅获得收益。今天将要教大家如何用虚拟信用卡在 Onlyfans 订阅HongkongDoll。</p>
<h2 id="Onlyfans是什么？"><a href="#Onlyfans是什么？" class="headerlink" title="Onlyfans是什么？"></a>Onlyfans是什么？</h2><p>OnlyFans是一个日益受欢迎的内容订阅服务平台，使创作者能够直接从他们的粉丝那里获得收入，也有很多不错的博主，比如 hongkongdoll 之类的哈哈。</p>
<p><strong>本文将教你如何使用虚拟卡在OnlyFans上充值和订阅博主。</strong></p>
<h1 id="一、准备工具："><a href="#一、准备工具：" class="headerlink" title="一、准备工具："></a>一、准备工具：</h1><ul>
<li>OnlyFans账号一个</li>
<li>虚拟信用卡（后续会进行详细教学具体教程在<a href="https://lcab-ljj.github.io/2024/07/16/Fomepay/">开卡教学</a>）： <a target="_blank" rel="noopener" href="https://gpt.fomepay.com/#/pages/login/index?d=YB57GG">开卡地址</a>( 使用这个链接复制到浏览器打开，2分钟即可搞定开卡，还可享受88折优惠~)</li>
</ul>
<h1 id="二、如何购买虚拟卡支付-Onlyfans？"><a href="#二、如何购买虚拟卡支付-Onlyfans？" class="headerlink" title="二、如何购买虚拟卡支付 Onlyfans？"></a>二、如何购买虚拟卡支付 Onlyfans？</h1><h2 id="2-1-首先，不建议大家去某宝各自店铺买"><a href="#2-1-首先，不建议大家去某宝各自店铺买" class="headerlink" title="2.1 首先，不建议大家去某宝各自店铺买"></a>2.1 首先，不建议大家去某宝各自店铺买</h2><p>为什么不建议大家去某宝店铺购买呢？我已经踩过坑了：花了30美金，按理来说换算下来就220块左右，结果呢？却被收取了280元。更重要的是，这张卡是临时性的，用完一次后就报废了。</p>
<p><img src="/images/loading.png" data-original="/images/HongKong1.jpg"></p>
<h2 id="2-2-推荐使用一些稳定的虚拟卡平台："><a href="#2-2-推荐使用一些稳定的虚拟卡平台：" class="headerlink" title="2.2 推荐使用一些稳定的虚拟卡平台："></a>2.2 推荐使用一些稳定的虚拟卡平台：</h2><p>比如这个：<a target="_blank" rel="noopener" href="https://gpt.fomepay.com/#/pages/login/index?d=YB57GG">https://gpt.fomepay.com/#/pages/login/index?d=YB57GG</a> ( 使用这个链接复制到浏览器打开)</p>
<p>此外，用这个链接还可以享受开卡88折优惠，开卡费总共100元左右，开的卡有效期达2年（平均每天不到两毛钱）。之后每次消费，你只需充值所需的美金就行，直接用支付宝支付，简单又方便。如果你充多了，还可以把多余的钱提现回来，完全不用担心跑路。</p>
<p><img src="/images/loading.png" data-original="/images/%E9%BB%91%E8%89%B2/20240619-%E5%BC%80%E5%8D%A1%E6%B5%81%E7%A8%8B_.jpg"></p>
<h1 id="三、开好虚拟卡后，如何支付OnlyFans？"><a href="#三、开好虚拟卡后，如何支付OnlyFans？" class="headerlink" title="三、开好虚拟卡后，如何支付OnlyFans？"></a>三、开好虚拟卡后，如何支付OnlyFans？</h1><p>首先支付宝往虚拟信用卡里充值订阅博主付费内容需要的金额（用多少充多少，不怕跑路）</p>
<p><img src="/images/loading.png" data-original="/images/%E9%BB%91%E8%89%B2/20240620-%E5%8D%A1%E7%89%87%E4%BD%99%E9%A2%9D%E5%A6%82%E4%BD%95%E6%8F%90%E7%8E%B0-2.png"></p>
<p>然后去onlyfans绑定虚拟卡，<a target="_blank" rel="noopener" href="https://onlyfans.com/my/payments/add_card">Onlyfans地址</a>：</p>
<p><img src="/images/loading.png" data-original="/images/hongkong2.jpg"></p>
<p>上面需要的每一项信息，都可以直接在虚拟卡网页上看到，直接复制过去即可：</p>
<p><img src="/images/loading.png" data-original="/images/hongkong3.jpg"></p>
<p>绑定支付信用卡后，就可以去订阅喜欢的博主啦，比如 hongkongdoll 之类的：</p>
<p><img src="/images/loading.png" data-original="/images/hongkong4.jpg"></p>
<hr>
<h1 id="你好，我是LCAB-LJJ！"><a href="#你好，我是LCAB-LJJ！" class="headerlink" title="你好，我是LCAB-LJJ！"></a>你好，我是LCAB-LJJ！</h1><p>亲爱的读者你好，我是LCAB-LJJ，一个专注于AI的研究生。 最新原创的文章都先发布在其他文章，欢迎关注哦~，</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lcab-ljj.github.io/2024/07/19/newsGPT4o/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LCAB-LJJ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIGC-platform">
      <meta itemprop="description" content="gpt-4">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AIGC-platform">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/19/newsGPT4o/" class="post-title-link" itemprop="url">【最新】OpenAI震撼发布全新模型，GPT-3.5光荣退役，引领大模型成本两年内狂降99%的革新</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-07-19 11:38:10 / 修改时间：11:42:36" itemprop="dateCreated datePublished" datetime="2024-07-19T11:38:10+08:00">2024-07-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>在科技界掀起新一波浪潮的OpenAI，近日突发重磅新模型——<strong>GPT-4o mini</strong>，这款模型不仅能力直逼原版GPT-4，更在价格上实现了惊人的飞跃，每百万输入tokens仅需15美分（约合1.09元人民币），每百万输出tokens也仅60美分（约合4.36元人民币），相比以往模型，成本大幅降低了一个数量级。</p>
<p><img src="/images/loading.png" data-original="/images/repalce.jpg"></p>
<p>随着GPT-4o mini的登场，昔日的明星模型GPT-3.5在ChatGPT应用中已正式退役，取而代之的是更为强大且经济的GPT-4o mini，免费用户现已能直接体验这一升级。同时，配套的API也已全面开放，支持高达128k的输入tokens（涵盖图像与文本），并突破性地支持16k输出tokens，远超近期竞争对手Claude 3.5 Sonnet的8k输出能力。</p>
<p>GPT-4o mini凭借其低成本、低延迟的特性，被官方强烈推荐用于多种高需求场景，包括链式或并行化模型调用、处理大规模上下文输入（如完整代码库或对话历史）以及实现快速、实时的文本交互（如客服系统）。更令人振奋的是，未来几天内，用户还将有机会参与GPT-4o mini的微调过程，进一步定制模型以满足个性化需求。</p>
<p>OpenAI CEO在回顾过去时提到，仅仅两年前，市场上最顶尖的模型还是GPT-3的text-davinci-003版本，不仅性能远不及今日，成本更是高达现在的100倍，这标志着大模型成本在短短两年内实现了99%的惊人降幅。</p>
<p>GPT-4o mini不仅在价格上占据优势，其性能同样不容小觑。在多项文本与多模态推理学术基准测试中，该模型均展现出超越其他小模型的实力，特别是在数学与代码能力上表现卓越，仅略逊于GPT-4o完整版。此外，GPT-4o mini还在函数调用方面展现出强大性能，为用户提供了更为全面的解决方案。</p>
<p>在价格方面，GPT-4o mini相较于竞争对手如Anthropic的Claude 3 Haiku和谷歌的Gemini Flash，更是拥有40%-60%的价格优势。这一优势使得GPT-4o mini在市场中更具竞争力，也进一步巩固了OpenAI在AI领域的领先地位。</p>
<p><img src="/images/loading.png" data-original="/images/v2-e0bfa882badfaaa773c7c5467d2f1609_180x120.jpg"></p>
<p>值得一提的是，GPT-4o mini已在多个实际应用场景中展现出其强大实力。通过与Ramp和Superhuman等公司的合作，该模型在“从文件中提取结构化数据”和“对邮件通信记录生成高质量回复”等任务中均表现出色，明显优于GPT-3.5 Turbo。此外，GPT-4o mini还曾在化名GPT-mini的情况下登上大模型竞技场供网友测试检验，并获得了6000+用户投票的认可，其表现与GPT-4 Turbo不相上下。</p>
<p>在安全性方面，GPT-4o mini也进行了全面升级。作为首个应用OpenAI指令层次结构方法的模型，该模型在抵抗越狱、Prompt注入和系统Prompt提取等方面表现出色。尽管在简单测试中难以通过一句话套出系统提示词，但在面对复杂越狱手段时仍需进一步加强防护。</p>
<p>此外，OpenAI CEO还透露了一个令人期待的消息：GPT-4o的实时语音模式《Her》将于本月晚些时候进入Alpha测试阶段，并将在稍晚时间正式发布。这一消息无疑为AI语音交互领域带来了新的期待。</p>
<p>然而，也有部分网友对GPT-4o mini的发布表示略感失望，他们更期待的是GPT-5的到来。不过，从GPT-4o mini的表现来看，它无疑已经为OpenAI的下一代模型奠定了坚实的基础。</p>
<p>在同一天内，DeepSeek与Mistral也分别取得了新进展。DeepSeek开源了DeepSeek-V2-0628版本，成功摘得大模型榜单上开源大模型的头名；而Mistral则推出了与英伟达合作开发的12B小模型，同样支持高达128K的上下文长度。这些进展无疑为AI领域注入了新的活力与可能。</p>
<p>那我们该怎么使用GPT4o呢？教程在我的宝藏小站其他文章~</p>
<p>喜欢我的文章就点点赞吧，欢迎同样热爱AIGC的朋友关注一起成长！</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lcab-ljj.github.io/2024/07/18/PVG/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LCAB-LJJ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIGC-platform">
      <meta itemprop="description" content="gpt-4">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AIGC-platform">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/18/PVG/" class="post-title-link" itemprop="url">PVG！以小博大</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-07-18 15:36:56 / 修改时间：15:40:59" itemprop="dateCreated datePublished" datetime="2024-07-18T15:36:56+08:00">2024-07-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>OpenAI于2024年7月18日凌晨发布了最新的技术研究——Prover-Verifier-Games（简称“PVG”），旨在解决AI模型的“黑盒”问题，提升其推理和输出准确性。该技术通过引入一种新的训练框架，使用<strong>小模型</strong>来验证和监督<strong>大模型</strong>的输出，从而提高整体的输出准确率和可控性。</p>
<p>具体来说，PVG框架包含两个主要组成部分：证明者（Prover）和验证者（ Verifier）。其中，证明者通常是一个更强大的模型，如GPT-4；而验证者则是一个相对较小且能力较弱的模型，如GPT-3。在这一过程中，证明者需要生成易于验证的解决方案，而验证者则负责检查这些解决方案的正确性。</p>
<p><img src="/images/loading.png" data-original="/images/pvg1.jpg"></p>
<p>这种博弈论框架不仅提高了语言模型输出的可读性和可验证性，还使得这些输出更容易被人类理解和评估。此外，这种方法也增强了模型的安全性和准确性，特别是在法律、金融和营销等对安全性和准确性要求极高的领域中。</p>
<p>实际上，PVG（Proof-Verifier Game）技术理念在2021年8月的一篇学术论文中已初露端倪，激发了OpenAI的创新灵感。这一技术框架根植于博弈论，通过模拟证明者（Prover）与验证者（Verifier）之间的交互博弈，旨在提升机器学习模型生成内容的准确性和质量。</p>
<p>在此机制下，证明者的核心职责是创造性地生成内容，而验证者则扮演着批判性评估的角色，负责判断这些内容的真实性与合理性。PVG的核心策略在于通过多轮次的迭代训练，不断强化证明者与验证者的能力。每轮训练中，验证者首先运用监督学习技术，从历史证明者生成的内容样本中，学习如何准确识别正确信息，增强辨别能力。</p>
<p>随后，证明者则借助强化学习策略，不断优化其生成策略，力求生成的内容既准确无误，又能有效说服验证者接受。这一过程中，证明者与验证者形成了紧密相依的关系，证明者需不断提升生成内容的说服力，而验证者则需持续精进其判别能力。</p>
<p>为了进一步提升训练效率与效果，OpenAI在PVG框架内引入了两种强化类型的证明者角色：有益证明者（Helpful Prover）与狡猾证明者（Sneaky Prover）。前者致力于生成既正确又具说服力的内容，以正面促进学习过程；后者则试图通过生成错误但极具迷惑性的内容，挑战验证者的判断力，从而促使验证者发展出更高的鲁棒性和准确性。</p>
<p><img src="/images/loading.png" data-original="/images/pvg2.jpg"></p>
<p>以实际案例为例，当面对复杂问题时（如涉及多步骤逻辑推理的数学问题），GPT-4等先进模型能够给出准确答案（如肖娜父亲年龄的解答），而验证者模型（如GPT-3）则需经历多轮验证，从阿莉亚的年龄出发，逐步推导出肖娜及其父亲的年龄，确保每一步推理的正确性。</p>
<p>此外，OpenAI还引入了“可检查性训练”方法，通过交替训练验证器与证明者模型，不仅增强了验证器对过往证明者生成解决方案的监督预测能力，还促使证明者根据验证器的反馈和解决方案的正确性进行持续自我优化。随着训练轮次的增加，验证器对狡猾证明者生成的错误信息的抵抗力显著增强，同时有益证明者生成的内容也变得更加清晰易懂，更贴近人类理解模式。</p>
<p>然而，值得注意的是，高效训练验证者模型依赖于大量真实、准确的标签数据，以确保其辨别能力的精准性。若验证模型本身存在偏差，仍有可能导致验证结果的非预期输出。因此，在推进PVG技术发展的过程中，持续优化数据质量与验证机制同样重要。</p>
<p>如果你喜欢我的分享就点点关注吧，我会持续更新最新新闻，欢迎喜欢AIGC的朋友看看我的其他文章</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lcab-ljj.github.io/2024/07/18/Sora/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LCAB-LJJ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIGC-platform">
      <meta itemprop="description" content="gpt-4">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AIGC-platform">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/18/Sora/" class="post-title-link" itemprop="url">【2024最新】4000字搞懂sora！一张脑图贯穿！</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-07-18 11:57:45 / 修改时间：13:55:14" itemprop="dateCreated datePublished" datetime="2024-07-18T11:57:45+08:00">2024-07-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>话不多说，上图！</p>
<p><img src="/images/loading.png" data-original="/images/%E8%84%91%E5%9B%BE.jpg"></p>
<p>博主到处查阅资料，凭借我自己的看法结合实际，下面就是对<strong>sora</strong>的具体阐释：</p>
<p>Sora是OpenAI推出的一款革命性的视频生成模型，能够根据文本指令、静态图像或视频生成长达60秒的完整视频。这一模型基于扩散式模型和自注意力深度学习机制，通过将视频片段转换为静态图像并去除噪音以达到清晰效果。</p>
<h1 id="1-核心技术与功能"><a href="#1-核心技术与功能" class="headerlink" title="1.核心技术与功能"></a>1.核心技术与功能</h1><h2 id="1-技术架构："><a href="#1-技术架构：" class="headerlink" title="1.技术架构："></a>1.技术架构：</h2><ul>
<li>Sora结合了Diffusion和Transformer技术，并融合了Google的MAGViT和DeepMind的NaViT等方案，应用了OpenAI DALL-E 3图像描述方案。</li>
<li>使用独特的CLIP模型架构，能够生成高质量的视频描述。</li>
<li>基于Transformer架构的扩散模型，可以灵活地扩展视频内容，改变风格和背景环境。</li>
</ul>
<h2 id="2-视频生成能力："><a href="#2-视频生成能力：" class="headerlink" title="2.视频生成能力："></a>2.视频生成能力：</h2><ul>
<li>能够生成高度细致的场景、复杂的多角度镜头以及富有情感的角色。</li>
<li>具备3D一致性、远距离相干性和物体持久性等模拟功能。</li>
<li>可以实现对复杂物理运动和逻辑关系的准确捕捉，尽管目前仍存在一些局限性。</li>
</ul>
<h2 id="3-应用范围："><a href="#3-应用范围：" class="headerlink" title="3.应用范围："></a>3.应用范围：</h2><ul>
<li>Sora在短视频、宣传片、动画电影等领域具有广泛的应用前景。</li>
<li>对广告业、电影预告片和短视频行业带来巨大影响，甚至可能颠覆这些行业。</li>
<li>在传媒领域推动智媒的发展，丰富元宇宙、长短视频和MR应用生态。</li>
</ul>
<h1 id="2-商业潜力与挑战"><a href="#2-商业潜力与挑战" class="headerlink" title="2.商业潜力与挑战"></a>2.商业潜力与挑战</h1><h2 id="1-商业化潜力："><a href="#1-商业化潜力：" class="headerlink" title="1.商业化潜力："></a>1.商业化潜力：</h2><p>Sora展现出明确的商业化潜力与应用路线，预计到2030年，全球&#x2F;中国相关市场复合增长率将达45%&#x2F;87%。<br>将进一步加深和拓宽OpenAI的护城河，少数巨头将占据底层算法和模型的主导地位。</p>
<h2 id="2-技术缺陷与挑战："><a href="#2-技术缺陷与挑战：" class="headerlink" title="2.技术缺陷与挑战："></a>2.技术缺陷与挑战：</h2><ul>
<li>目前Sora在处理复杂物理运动或逻辑关系时可能存在局限性，例如混淆文字表达或不符合现实世界的物理关系认知等。</li>
<li>需要更丰富的数据和更强的算力来优化其性能。</li>
<li>存在监管难题和版权、隐私等问题，需要与各方合作确保安全使用。</li>
</ul>
<h1 id="3-行业影响与未来趋势"><a href="#3-行业影响与未来趋势" class="headerlink" title="3.行业影响与未来趋势"></a>3.行业影响与未来趋势</h1><h2 id="1-行业影响："><a href="#1-行业影响：" class="headerlink" title="1.行业影响："></a>1.行业影响：</h2><ul>
<li><p>Sora的出现为视频领域带来了巨大的想象空间，突破了人类在专业能力上的限制，被誉为“世界模拟器”。</p>
</li>
<li><p>对于没有成熟运营、设计、策划团队的中小商家来说，Sora的智能生成可以实现内容生产的低成本化。</p>
</li>
</ul>
<h2 id="2-未来趋势："><a href="#2-未来趋势：" class="headerlink" title="2.未来趋势："></a>2.未来趋势：</h2><ul>
<li>随着业内追赶态势，市场上可能会出现更多类似Sora的模型和产品，促进用户采用率和需求的进一步增长。</li>
<li>Sora有望撬动AI多模态应用新热度，对传媒领域带来存量提质增效和新增应用场景。</li>
</ul>
<p>由此可见，Sora作为一款先进的视频生成模型，在技术架构、视频生成能力和应用范围等方面都展现了强大的潜力和优势。然而，它也面临着一些技术和监管上的挑战，需要持续优化和改进以实现更广泛的应用和更大的商业价值。</p>
<p><strong>那么我们不禁要问：Sora视频生成模型的技术细节和原理是什么？</strong></p>
<p>我认为，Sora视频生成模型是OpenAI于2024年2月16日发布的一项革命性技术，旨在通过文本提示、静态图像或现有视频生成或扩展高质量的视频内容。该模型在多个方面展现了显著的技术优势和创新。</p>
<h1 id="4-技术细节与原理"><a href="#4-技术细节与原理" class="headerlink" title="4.技术细节与原理"></a>4.技术细节与原理</h1><h2 id="1-扩散模型与Transformer架构"><a href="#1-扩散模型与Transformer架构" class="headerlink" title="1. 扩散模型与Transformer架构"></a>1. 扩散模型与Transformer架构</h2><p>Sora基于扩散模型（Diffusion Model），其核心机制是从一个看起来像静态噪声的视频开始，逐步去除噪声，最终生成清晰的视频。这种模型能够处理视频和图片中时空片段的潜在代码，并利用Transformer架构来捕捉前后文全局关系，从而实现每一帧图像的精确生成以及前后时空的一致性。</p>
<h2 id="2-视频补丁（Patch）"><a href="#2-视频补丁（Patch）" class="headerlink" title="2. 视频补丁（Patch）"></a>2. 视频补丁（Patch）</h2><p>Sora使用了“视频补丁”（Patch）这一高度可扩展且有效的表示形式。视频补丁将视频数据转化为较小的数据单元，类似于GPT中的token，这使得模型能够在不同类型的视频和图像上进行训练。这些补丁作为数据的有效提示，帮助模型更好地理解和生成复杂的视觉场景。</p>
<h2 id="3-大规模训练与语言理解"><a href="#3-大规模训练与语言理解" class="headerlink" title="3. 大规模训练与语言理解"></a>3. 大规模训练与语言理解</h2><p>Sora采用了大规模训练的方法，并结合了DALL·E 3中的重述技术和ChatGPT的大语言模型，以提高模型的语言理解能力。具体来说，Sora利用重新字幕技术生成高度描述性的字幕，并将简短的用户提示转换为详细的描述，从而生成与提示更匹配的高质量视频。</p>
<h2 id="4-DiT模型与解码器"><a href="#4-DiT模型与解码器" class="headerlink" title="4. DiT模型与解码器"></a>4. DiT模型与解码器</h2><p>Sora的核心技术之一是DiT（Denoising Diffusion Transformer）模型。该模型将视频压缩到低维潜在空间中，并将其分解为补丁，然后在低维空间中进行训练。通过逐步添加高斯噪声并学习如何逆向去除噪声，DiT模型能够生成新数据。最后，模型通过对应的解码器，将生成的元素映射回像素空间，完成视频生成任务。</p>
<h2 id="5-功能特点"><a href="#5-功能特点" class="headerlink" title="5. 功能特点"></a>5. 功能特点</h2><ul>
<li><strong>长时长视频生成</strong>：Sora可以一次性生成长达60秒的高保真视频，这在当前的AI视频生成领域中是一个重大突破。<br>复杂场景与细节：Sora能够生成包含精细复杂场景、生动角色表情以及复杂镜头运动的视频，确保三维空间中的人物和场景元素保持一致性。</li>
<li><strong>多模态输入</strong>：除了文本提示外，Sora还支持根据静态图像或现有视频进行扩展和生成。<br>灵活采样与全分辨率输出：Sora具有灵活采样和全分辨率输出的功能，可以快速创建不同设备的原始宽高比内容。</li>
</ul>
<h1 id="5-小结"><a href="#5-小结" class="headerlink" title="5.小结"></a>5.小结</h1><p>我觉得，Sora视频生成模型通过结合扩散模型、Transformer架构、视频补丁技术、大规模训练和语言理解能力等先进技术，实现了在视频生成领域的多项突破。</p>
<h1 id="6-Sora在处理复杂物理运动和逻辑关系时的具体局限性有哪些？"><a href="#6-Sora在处理复杂物理运动和逻辑关系时的具体局限性有哪些？" class="headerlink" title="6.Sora在处理复杂物理运动和逻辑关系时的具体局限性有哪些？"></a>6.Sora在处理复杂物理运动和逻辑关系时的具体局限性有哪些？</h1><h2 id="1-Sora在处理复杂物理运动和逻辑关系时存在以下具体局限性："><a href="#1-Sora在处理复杂物理运动和逻辑关系时存在以下具体局限性：" class="headerlink" title="1.Sora在处理复杂物理运动和逻辑关系时存在以下具体局限性："></a>1.Sora在处理复杂物理运动和逻辑关系时存在以下具体局限性：</h2><p><strong>1.无法准确模拟复杂物理现象</strong>：尽管Sora能够理解用户指令并生成视频，但其在模拟复杂场景中的物理特性方面仍存在困难。例如，它可能难以准确模拟玻璃杯倾倒、食物咬痕等复杂的物理运动，并且无法推演时间变化。<br><strong>2.混淆因果关系和空间细节</strong>：Sora有时会创造出不符合现实世界物理关系认知的画面，特别是在处理复杂、繁琐的物理运动时，可能无法准确模拟因果关系或推演时间变化。此外，该模型还存在混淆部分画面中文字表达的可能性，如广告牌标语不合逻辑或不成文字。<br><strong>3.难以精确描述随时间变化的事件</strong>：Sora可能无法准确模拟复杂场景的物理原理，并且可能无法理解因果关系，混淆提示的空间细节，难以精确描述随着时间推移发生的事件。<br><strong>4.对牛顿定律等物理规律的掌握不足</strong>：一些外部专家猜测，Sora很难将物理世界中的牛顿定律、湍流方程和量子学定理等规律一条一条在模型中显式罗列出来，这可能是由于神经网络模型的涌现之力所限。<br><strong>5.视频时长限制</strong>：Sora生成的视频时长有限制，最长只能生成60秒的视频，对于更长的视频片段，Sora会使用预训练模型进行处理。</p>
<h1 id="7-目前OpenAI如何解决Sora模型在数据隐私和版权方面的挑战？"><a href="#7-目前OpenAI如何解决Sora模型在数据隐私和版权方面的挑战？" class="headerlink" title="7.目前OpenAI如何解决Sora模型在数据隐私和版权方面的挑战？"></a>7.目前OpenAI如何解决Sora模型在数据隐私和版权方面的挑战？</h1><p>目前，OpenAI在解决Sora模型在数据隐私和版权方面的挑战方面采取了多种措施。首先，尽管有报道指出Sora模型的数据集可能包括未经许可获取的大量书籍及其他版权材料，引发了关于是否遵守知识产权法和数据采集伦理标准的争议，但OpenAI声称在训练Sora时使用了“公开可用”和“已许可”的内容。</p>
<p>为了应对这些挑战，OpenAI采取了一系列安全措施和对抗性测试。例如，在产品中使用Sora前，OpenAI承诺将由专家对模型进行对抗性测试，以评估其危害或风险，并核查并拒绝包含极端暴力、性骚扰、歧视、恐怖主义、仇恨图像、他人IP等文本输出的内容。此外，Sora内置的文本提示过滤器可以筛选发送给模型的所有提示，阻止对暴力、色情、仇恨言论以及名人肖像等敏感或不适当内容的请求。视频内容过滤器也能检查生成的视频帧，屏蔽违反OpenAI安全政策的内容。</p>
<p>虽然OpenAI已经采取了一些措施来缓解数据隐私和版权方面的挑战，但仍有公众对其处理敏感数据问题上的透明度和有效性表示担忧。</p>
<h1 id="8-Sora对广告业、电影预告片和短视频行业的具体影响有哪些实例？"><a href="#8-Sora对广告业、电影预告片和短视频行业的具体影响有哪些实例？" class="headerlink" title="8.Sora对广告业、电影预告片和短视频行业的具体影响有哪些实例？"></a>8.Sora对广告业、电影预告片和短视频行业的具体影响有哪些实例？</h1><h2 id="1-Sora对广告业、电影预告片和短视频行业的具体影响主要体现在以下几个方面："><a href="#1-Sora对广告业、电影预告片和短视频行业的具体影响主要体现在以下几个方面：" class="headerlink" title="1.Sora对广告业、电影预告片和短视频行业的具体影响主要体现在以下几个方面："></a>1.Sora对广告业、电影预告片和短视频行业的具体影响主要体现在以下几个方面：</h2><ul>
<li>降低视频制作成本和门槛：Sora能够帮助直播卖家高效地从长达数小时的直播中剪辑出亮点，并进行混剪和配音等处理，实现内容生产的低成本化。此外，Sora降低了视频制作的门槛和成本，将颠覆广告业、电影预告片、短视频行业和游戏等领域。</li>
<li>提高内容创作质量和效率：Sora的出现极大地提升了短视频的内容供给和创作质量，可能使短剧重心回归高质量剧本创作。它还能快速、准确地生成生动的现场视频，提高新闻报道的时效性。</li>
<li>个性化广告推送：在广告推送方面，Sora可以通过收集特定用户的偏好和兴趣来定制专属广告内容，提高顾客购买欲望和品牌忠诚度。</li>
<li>电影预告片和社交媒体宣传视频的制作：对于即将上映的电影或电视节目，使用Sora可以简化预告片或社交媒体宣传视频的制作过程，只需输入关键情节或场景的简短描述便能渲染出精选内容汇编的短视频，缩短制作周期的同时还能节省一定的制作成本。</li>
<li>促进技术创新和应用想象：Sora代表了AI赋能的新阶段，将为自学发展、人才培训、科学研究、产品研发等多个领域带来技术革新和应用想象。</li>
<li>改变内容产业的成本结构和资源支撑体系：短期内，Sora将直接改变很多内容产业的成本结构以及资源支撑体系，长期来看，其构建的基于三维物理世界来创造数字原型的强大引擎，将给一些产业带来深远影响。</li>
<li>可能引发行业内的就业变化：虽然Sora的问世可能会导致设计师、摄影师和后期制作岗位需求的大量减少，但对于拥有自身风格和调性的创作者来说，Sora只能起到辅助作用。同时，也有观点认为，尽管Sora带来了效率上的提升，但并不是说不需要人了，视频行业还有很多的环节不能被替代，比如创意。</li>
</ul>
<h1 id="9-未来市场上类似Sora的视频生成模型有哪些，它们的主要区别和优势是什么？"><a href="#9-未来市场上类似Sora的视频生成模型有哪些，它们的主要区别和优势是什么？" class="headerlink" title="9.未来市场上类似Sora的视频生成模型有哪些，它们的主要区别和优势是什么？"></a>9.未来市场上类似Sora的视频生成模型有哪些，它们的主要区别和优势是什么？</h1><h2 id="1-未来市场上类似Sora的视频生成模型主要有以下几款：Runway和Pika。这些模型与Sora的主要区别和优势如下："><a href="#1-未来市场上类似Sora的视频生成模型主要有以下几款：Runway和Pika。这些模型与Sora的主要区别和优势如下：" class="headerlink" title="1.未来市场上类似Sora的视频生成模型主要有以下几款：Runway和Pika。这些模型与Sora的主要区别和优势如下："></a>1.未来市场上类似Sora的视频生成模型主要有以下几款：Runway和Pika。这些模型与Sora的主要区别和优势如下：</h2><h3 id="1-视频时长："><a href="#1-视频时长：" class="headerlink" title="1.视频时长："></a>1.视频时长：</h3><ul>
<li>Sora：能够生成长达60秒的视频，这是目前市场上其他模型难以匹敌的。</li>
<li>Runway和Pika：虽然具体时长未明确提及，但它们在视频时长方面可能不如Sora长，通常只能生成5秒以内的短视频。</li>
</ul>
<h3 id="2-场景复杂度和逼真度："><a href="#2-场景复杂度和逼真度：" class="headerlink" title="2.场景复杂度和逼真度："></a>2.场景复杂度和逼真度：</h3><ul>
<li>Sora：可以生成主题精确、背景细节复杂的场景，并且视频效果逼真。此外，Sora还能够实现多角度镜头切换，保持前后一致性。</li>
<li>Runway和Pika：相比之下，它们在处理复杂场景和多角度镜头方面的能力可能较弱，无法生成如此高质量和细节丰富的视频。</li>
</ul>
<h3 id="3-物理规律的掌握："><a href="#3-物理规律的掌握：" class="headerlink" title="3.物理规律的掌握："></a>3.物理规律的掌握：</h3><ul>
<li>Sora：在对物理规律的掌握方面表现不俗，例如在汽车行驶视频中，汽车影子与车身始终契合。</li>
<li>Runway和Pika：这方面的能力可能不如Sora强，因为它们缺乏足够的数据标注和清洗工作量，导致模型在逻辑性和连续性方面的表现不如Sora。</li>
</ul>
<h3 id="4-多模态能力："><a href="#4-多模态能力：" class="headerlink" title="4.多模态能力："></a>4.多模态能力：</h3><ul>
<li>Sora：不仅支持文本生成视频，还具备图像生成视频等能力，并能执行各种图像和视频编辑任务。</li>
<li>Runway和Pika：虽然也具备一定的多模态能力，但在综合应用和扩展性方面可能不如Sora全面。</li>
</ul>
<h3 id="5-技术架构："><a href="#5-技术架构：" class="headerlink" title="5.技术架构："></a>5.技术架构：</h3><ul>
<li>Sora：采用转化器架构上的扩散模型，能够自主提取信息之间的关联并进行统一化重组，从而实现对文本信息的精准捕捉和渲染。</li>
<li>Runway和Pika：具体的技术架构未详细说明，但可能没有像Sora那样先进的技术支撑。</li>
</ul>
<h3 id="6-安全性和伦理风险："><a href="#6-安全性和伦理风险：" class="headerlink" title="6.安全性和伦理风险："></a>6.安全性和伦理风险：</h3><ul>
<li>Sora：尽管存在安全伦理风险，但其领先优势难以被打破，促使社交及内容平台与OpenAI更加紧密地合作。</li>
<li>Runway和Pika：同样面临安全性和伦理风险的问题，但可能在应对措施和风险管理方面不如Sora成熟。</li>
</ul>
<p>Sora在视频时长、场景复杂度、物理规律掌握、多模态能力和技术架构等方面均具有显著优势，而Runway和Pika则在这些方面略显不足。</p>
<p>以上内容就是我的看法，欢迎对ai前沿感兴趣的朋友看看博主其他文章博主的宝藏小站</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lcab-ljj.github.io/2024/07/17/%E9%92%88%E5%B0%96%E5%AF%B9%E9%BA%A6%E8%8A%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LCAB-LJJ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIGC-platform">
      <meta itemprop="description" content="gpt-4">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AIGC-platform">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/17/%E9%92%88%E5%B0%96%E5%AF%B9%E9%BA%A6%E8%8A%92/" class="post-title-link" itemprop="url">针尖对麦芒！Anthropic 推出 Claude Android 可实时翻译！</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-07-17 11:40:46 / 修改时间：15:11:27" itemprop="dateCreated datePublished" datetime="2024-07-17T11:40:46+08:00">2024-07-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>Anthropic，作为OpenAI的强劲对手，于本周二正式推出了专为Android用户设计的Claude应用程序，</strong>旨在通过拓宽Claude的接入平台，吸引用户从ChatGPT转向其服务。这款Android应用承袭了五月问世的iOS版本的设计理念，用户无需支付任何费用即可体验到Anthropic顶尖AI模型——Claude 3.5 Sonnet的强大功能，并可选择升级为Anthropic的Pro或Team订阅计划以享受更多高级服务。</p>
<p><img src="/images/loading.png" data-original="/images/b9aeb9456bf5cea752748e69500fea5.png"></p>
<p>应用内，用户能够无缝同步跨设备的Claude对话记录，还能即时上传图片或文档，利用应用程序内置的实时图像分析功能进行深度处理。尤为值得一提的是，Claude Android版还集成了实时语言翻译特性，Anthropic寄望这一功能成为吸引用户频繁使用应用的强大诱因。此外，该应用还为企业客户量身打造了移动访问权限，使他们能够随时随地管理自己的Claude账户。</p>
<p><strong>然而，</strong>尽管Anthropic自信其AI技术能与OpenAI及Google的产品分庭抗礼，市场反应却揭示了其在吸引大众用户方面的挑战。回顾Claude iOS应用的初次亮相，其市场接受度较为温和，首周全球下载量仅为157,000次，相比之下，ChatGPT iOS应用在发布的前五天便实现了480,000次的安装量，这一数据对比凸显了Anthropic在消费者市场中的推广难题。</p>
<p><img src="/images/loading.png" data-original="http://eb118-file.cdn.bcebos.com/upload/28da87b7ec344be799292f9b99916a88_158473674.png"></p>
<p><strong>欢迎热爱ai技术的道友看看博主其他文章参与讨论哦</strong></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lcab-ljj.github.io/2024/07/16/Fomepay/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LCAB-LJJ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIGC-platform">
      <meta itemprop="description" content="gpt-4">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | AIGC-platform">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/16/Fomepay/" class="post-title-link" itemprop="url">【2024最新版】怎么购买GPT-4？GPT-4买不了怎么办？GPT-4订阅银行卡教程他来了！！！</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-07-16 16:23:08" itemprop="dateCreated datePublished" datetime="2024-07-16T16:23:08+08:00">2024-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-07-17 09:34:40" itemprop="dateModified" datetime="2024-07-17T09:34:40+08:00">2024-07-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>0.前言</p>
<p>1.chatGPT介绍</p>
<p>2.Fomepay常见问题分析</p>
<p>3.如何用Fomepay来开通GPT4.0</p>
<p>*什么是虚拟信用卡<br>*如何开通虚拟信用卡</p>
<p>4.Fomepay支持哪些应用？</p>
<p>5.如何查看卡资料</p>
<h3 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h3><p><a href="https://link.zhihu.com/?target=https://gpt.fomepay.com/%23/pages/login/index?d=YB57GG">Fomepay</a>虚拟信用卡专注于提供快速、安全的在线支付解决方案。</p>
<p>简单的说来，就是如果你订阅国外的服务（如chatgpt或者onlyfans）的话，需要用到国外的信用卡。而FomePay就可以给你提供一个国外的虚拟信用卡，让你可以去订阅相关服务。</p>
<h3 id="1-chatGPT介绍"><a href="#1-chatGPT介绍" class="headerlink" title="1.chatGPT介绍"></a>1.chatGPT介绍</h3><p>ChatGPT3.5自从发布以来，便受到了广泛的关注和火热追求。而作为3.5升级版的GPT4.0，比3.5会更加稳定、没有字数限制、回答更准确以及支持AI绘图等待，更是受到了大众的欢迎和追捧。</p>
<p>但受到某些原因限制，在国内并不能直接用上GPT4.0，但这么好的生产力工具，怎么只能让洋人使用呢?洋人会的，我也要会!正所谓师夷长技以制夷也。</p>
<p>所幸，经过一番摸索和尝试以及失败，我终于找到了一条能稳定、安全、快捷的用上GPT4.0的方法，那就是：通过虚拟卡来开通GPT订阅。</p>
<h3 id="2-Fomepay常见问题分析"><a href="#2-Fomepay常见问题分析" class="headerlink" title="2.Fomepay常见问题分析"></a>2.Fomepay常见问题分析</h3><p> <a target="_blank" rel="noopener" href="https://gpt.fomepay.com/#/pages/login/index?d=YB57GG">Fomepay</a>有如下特性：</p>
<p>1.隐私性：支持手机号或者邮箱注册<br>2.快捷性：支持微信或支付宝支付<br>3.费用低：开卡费为10美元，有效期最高4年，并且无年费&#x2F;月费。<br>4.卡片多：Fomepay支持数十种不同类型的卡片，几乎涵盖所有你需要用到的服务，你可以根据自己需求选择不同卡片类型。</p>
<h3 id="3-如何用Fomepay来开通GPT4-0"><a href="#3-如何用Fomepay来开通GPT4-0" class="headerlink" title="3.如何用Fomepay来开通GPT4.0"></a>3.如何用Fomepay来开通GPT4.0</h3><p><strong>3.1 什么是虚拟信用卡</strong></p>
<p>简单的说来，就是如果你需要订阅国外的服务(如chatgpt或者onlyfans)的话，就必须要有国外的信用卡。但是国内不是没有办法开通国外信用卡吗，所以就需要使用虚拟信用卡，然后用这张虚拟卡去订阅服务。</p>
<p>这里我比较推荐虚拟卡平台<a target="_blank" rel="noopener" href="https://gpt.fomepay.com/#/pages/login/index?d=YB57GG">Fomepay</a></p>
<p><strong>3.2 如何开通虚拟卡</strong></p>
<p>首先点击链接进入：<a target="_blank" rel="noopener" href="https://gpt.fomepay.com/#/pages/login/index?d=YB57GG">Fomepay一分钟开卡，轻松订阅海外服务！！！</a></p>
<p>进入主页，通过手机号码注册一个账号（也可以通过邮箱注册，注册的账号是一模一样的）。</p>
<p><img src="/images/loading.png" data-original="https://pic3.zhimg.com/80/v2-23518dd6b4153096ff53b51265dc723e_720w.webp" alt="photo1"></p>
<p>登录后进入首页，这里可以看到很多不同种类的卡。这些卡的区别在于：它们是由不同机构发行的。</p>
<p>因此，不同的卡支持不同的服务，我们在开通的时候，可以点进卡片详情页面，来确认该类型的卡片是否支持我们需要订阅的服务（如果没说不支持某种服务的话，那就是默认支持的）。</p>
<p><img src="/images/loading.png" data-original="https://pic3.zhimg.com/80/v2-8c6ba8f088ef68c65e8f08759b256612_720w.webp" alt="photo2"></p>
<p>点击某一个想要的卡片后，进入开卡页面。这里我们需要充值一定金额（单位是美元，有些地方写的是流量，意思也是美元），充值的金额中包含了开卡费，多余的费用则会自动转为卡片余额。</p>
<p><img src="/images/loading.png" data-original="https://pic2.zhimg.com/80/v2-c84bc2e6f62ed5ff482b194e9ebc44e1_720w.webp" alt="photo3"></p>
<p>开卡时一定要注意该卡适用的平台和禁止使用的场景，由于卡段很多，所以需要花些时间找一下。如果没有明确说明某个平台不能用，那一般就是可以用的意思。</p>
<p><img src="/images/loading.png" data-original="https://pic1.zhimg.com/80/v2-784516a677b19444644c69271afb7ef4_720w.webp" alt="photo4"></p>
<p>最后选择支付方式，填写姓名，再点击申请开卡。支付完成之后在首页就可以看到卡，点击更多操作–CVC安全码可以看到卡的详细资料（卡号、安全码、有效期、账单地址）</p>
<p>备注：点击CVC的时候，会需要输入支付密码，在<strong>我的–设置–安全–设置支付密码</strong>即可。</p>
<p><img src="/images/loading.png" data-original="https://pic3.zhimg.com/80/v2-845809857bd1bedf268375cc6cb54b46_720w.webp" alt="photo5"></p>
<p><img src="/images/loading.png" data-original="https://pic1.zhimg.com/80/v2-90f1e8eae401925e436f388f2e61476c_720w.webp" alt="photo6"></p>
<p>订阅应用的时候只需要按照卡资料填写就能完成支付。</p>
<h3 id="4-Fomepay支持哪些应用？"><a href="#4-Fomepay支持哪些应用？" class="headerlink" title="4.Fomepay支持哪些应用？"></a>4.Fomepay支持哪些应用？</h3><p>适用范围非常广泛，只要是支持美国VISA卡的场景均可。以下都是可以成功支付应用。只列举部分例如:</p>
<p>推荐实用场景：ChatGPT、Midjourney、Openai、AppStore.GooglePlay、Onlyfans.</p>
<p>AI软件:ChatGPT、Midjourney、Notion、Linode、Openai、Premium</p>
<p>电商网站:Amazon.WishEbay.速卖通、敦煌网、阿里巴巴国际、Paypal、Eaby、Stripe商户、Supreme、Nike、Etsy支付</p>
<p>应用商店:AppStore.GooglePlay、Onlyfans.github、美区苹果商店apple id、RapidAPI订阅、Pixiv付费会员</p>
<p>视频网站:Netflix.Amazon.PrimeViedo、Hulu、Cloudflare、Warp验证</p>
<p>时尚潮流:FootLocker.Yeezy、Supply SupremeNIKE、Stockx、Telegram会员等</p>
<p>其他应用:T-Mobile充值、GoogleVoice充值、UltraMobile充值、谷歌云、微软云、阿里云国际、ccBill等。</p>
<h3 id="5-如何查看卡资料"><a href="#5-如何查看卡资料" class="headerlink" title="5.如何查看卡资料"></a>5.如何查看卡资料</h3><p>在平台首页点击我的卡片–CVC安全码进去就可以看到卡的详细资料。有效期是月在前&#x2F;年在后。除了开卡费用是不需要月费、年费的~</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>开卡地址：<a target="_blank" rel="noopener" href="https://gpt.fomepay.com/#/pages/login/index?d=YB57GG">Fomepay开卡地址</a></p>
<p>总费用：</p>
<p>*开卡费用10美元（这是2年的费用，平均每天不到1毛钱）<br>*你充值的金额，用多少充多少就行。</p>
<h4 id="我是一个热爱技术的博主，喜欢就加个关注一起成长吧！"><a href="#我是一个热爱技术的博主，喜欢就加个关注一起成长吧！" class="headerlink" title="我是一个热爱技术的博主，喜欢就加个关注一起成长吧！"></a>我是一个热爱技术的博主，喜欢就加个关注一起成长吧！</h4>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">LCAB-LJJ</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,d=o();function o(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=o());for(var e,i=0;i<d.length;i++)0<=(e=(e=d[i]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,a,n,o=d[i];e=function(){d=d.filter(function(t){return o!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(o)},(t=o).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,n=t.getAttribute("data-original"),a.onload=function(){t.src=n,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=n},t.src!==n&&(a.src=n)))}()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)}(this);</script></body>
</html>
